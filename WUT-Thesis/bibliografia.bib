% Artykuł w recenzowanym czasopiśmie.
@article{szczypiorski2015,
    author    = {Szczypiorski, K. and Janicki, A. and Wendzel, S},
    title     = {{T}he {G}ood, {T}he {B}ad {A}nd {T}he {U}gly: {E}valuation of {W}i-{F}i {S}teganography},
    journal   = {Journal of Communications},
    volume    = {10},
    number    = {10},
    pages     = {747--752},
    publisher = {Journal of Communications (JCM)},
    year      = {2015}
}

% Książka.
@book{goossens93,
    author    = {Michel Goossens and Frank Mittelbach and Alexander Samarin},
    title     = {The LaTeX Companion},
    publisher = {Addison-Wesley},
    address   = {Reading, Massachusetts},
    year      = {1993}
}

% Fragment książki (np. zakres stron).
@inbook{wang97,
    author    = {Hao Wang},
    title     = {A Logical Journey: From G{\"o}del to Philosophy.},
    publisher = {A Bradford Book},
    pages     = {316},
    year      = {1997}
}

% Fragment książki (np. esej), posiadający własny tytuł.
@incollection{goedel95,
    author    = {Kurt G{\"o}del},
    title     = {Texts relating to the ontological proof},
    booktitle = {Unpublished Essays and Lectures},
    publisher = {Oxford University Press},
    pages     = {429--437},
    year      = {1995}
}

% Publikacja konferencyjna.
@inproceedings{benzmuller2014,
    author       = {{Ch}. Benzmuller and B. W. Paleo},
    title        = {Automating {G\"o}del’s {O}ntological {P}roof of {G}od’s {E}xistence with {H}igher-order {A}utomated {T}heorem {P}rovers},
    booktitle    = {European	Conference on Artificial Intelligence},
    publisher    = {IOS Press},
    howpublished = {Dostęp zdalny (10.04.2019): \url{http://page.mi.fu-berlin.de/cbenzmueller/papers/C40.pdf}},
    year         = {2014}
}

% Raport techniczny.
@techreport{duqu2011,
    author      = {Bencsáth, B. and Pék, G. and Buttyán, L. and Félegyházi M.},
    title       = {{D}uqu: {A} {S}tuxnet-like malware found in the wild},
    institution = {Laboratory of Cryptography and System Security, Hungary},
    year        = {2011}
}

% Specyfikacja techniczna.
@manual{shs2015,
    title        = {{FIPS} 180-4: {S}ecure {H}ash {S}tandard ({SHS})},
    howpublished = {Dostęp zdalny (13.03.2019): \url{https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf}},
    year         = {2015}
}

% Praca magisterska.
@mastersthesis{wozniak2018,
    author = {Woźniak, Piotr},
    title  = {{P}rogramowanie kwadratowe w usuwaniu efektu rozmycia ruchu w fotografii cyfrowej},
    school = {Wydział Elektroniki i Technik Informacyjnych, Politechnika Warszawska},
    year   = {2018}
}

% Nieopublikowany artykuł, dostępny np. tylko w internecie.
@unpublished{koons2005,
    author = {Koons, Robert C.},
    title  = {{S}obel on {G\"o}del’s {O}ntological {P}roof},
    note   = {Dostęp zdalny (25.04.2019): \url{http://www.robkoons.net/media/69b0dd04a9d2fc6dffff80b4ffffd524.pdf}},
    year   = {2005}
}

% Źródło innego typu, np. repozytorium na GitHubie.
@misc{dcp19,
    author       = {Brodzki, Artur M.},
    title        = {{I}mplementation of own steganography protocol {DCP}-19, loosely based on {HICCUPS}},
    howpublished = {Dostęp zdalny (14.03.2019): \url{https://github.com/ArturB/DCP-19}},
    year         = {2019}
}



% -----------------------------------------------------
@misc {japan_ai_2016,
    author       = {{Ministry of Internal Affairs and Communications, Japan}},
    title        = {{White Paper on Information and Communications in Japan 2016}},
    howpublished = {Dostęp zdalny (2025-11-14): \url{https://www.soumu.go.jp/johotsusintokei/whitepaper/eng/WP2016/2016-index.html}},
    year         = {2016}
}

% Vision Transformer
@inproceedings{vit_dosovitskiy2021,
    author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
    title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    booktitle = {International Conference on Learning Representations (ICLR)},
    year      = {2021},
    howpublished = {Dostęp zdalny (2025-11-14): \url{https://arxiv.org/abs/2010.11929}}
}

% ONNX Runtime
@inproceedings{onnx_bai2019,
    author    = {Bai, Junjie and Lu, Fang and Zhang, Ke and others},
    title     = {{ONNX}: Open Neural Network Exchange},
    booktitle = {GitHub repository},
    year      = {2019},
    howpublished = {Dostęp zdalny (2025-11-14): \url{https://github.com/onnx/onnx}}
}
% ONNX (peer-reviewed reference discussing ONNX as a model exchange format)
@inproceedings{onnx_icsme_openja2022,
    author    = {Openja, Moses and Nikanjam, Amin and Haj Yahmed, Ahmed and Khomh, Foutse and Jiang, Zhen Ming},
    title     = {An Empirical Study of Challenges in Converting Deep Learning Models},
    booktitle = {2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
    year      = {2022},
    publisher = {IEEE},
    doi       = {10.1109/ICSME55016.2022.00032},
}

% TensorFlow Lite Quantization
@article{tflite_quantization_krishnamoorthi2018,
    author  = {Krishnamoorthi, Raghuraman},
    title   = {Quantizing deep convolutional networks for efficient inference: A whitepaper},
    journal = {arXiv preprint arXiv:1806.08342},
    year    = {2018},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/1806.08342}}
}

% TinyML - on-device machine learning
@article{tinyml_reddi2021,
    author  = {Reddi, Vijay Janapa and Plancher, Brian and Kennedy, Susan and Moroney, Laurence and Warden, Pete and Agarwal, Anant and Banbury, Colby and Banzi, Massimo and Bennett, Matthew and Carney, Nat and Chitnis, Nirvaana and Croce, Cristian and Kauffmann, Amy and Kiriu, Ian and Lin, Tiny Yiu-Leung and Patterso, Dale and Seshadri, Sandeep and Singh, Inderjit and Sordo, Jaime and Wilson, Evelyn},
    title   = {Widening Access to Applied Machine Learning with TinyML},
    journal = {arXiv preprint arXiv:2106.04008},
    year    = {2021},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2106.04008}}
}

% Edge Computing
@article{edge_computing_shi2016,
    author  = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
    title   = {Edge Computing: Vision and Challenges},
    journal = {IEEE Internet of Things Journal},
    volume  = {3},
    number  = {5},
    pages   = {637--646},
    year    = {2016},
    publisher = {IEEE}
}

% Edge vs Cloud comparison
@article{edge_vs_cloud_mao2017,
    author  = {Yuyi Mao and Changsheng You and Jun Zhang and Kaibin Huang and Khaled Letaief},
    title   = {A Survey on Mobile Edge Computing: The Communication Perspective},
    journal = {IEEE Communications Surveys \& Tutorials},
    volume  = {19},
    number  = {4},
    pages   = {2322--2358},
    year    = {2017},
    publisher = {IEEE}
}

% AI Market Precedence Research 2025
@misc{ai_market_precedence_2025,
    author       = {{Precedence Research}},
    title        = {Artificial Intelligence (AI) Market Size, Share and Trends 2025 to 2034},
    year         = {2025},
    note         = {Report Code: 1635},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://www.precedenceresearch.com/artificial-intelligence-market}}
}

% Image Captioning Survey
@article{image_captioning_survey_hossain2019,
    author  = {Hossain, Md Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
    title   = {A Comprehensive Survey of Deep Learning for Image Captioning},
    journal = {ACM Computing Surveys},
    volume  = {51},
    number  = {6},
    pages   = {1--36},
    year    = {2019},
    publisher = {ACM}
}

% Show and Tell
@inproceedings{show_and_tell_vinyals2015,
    author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
    title     = {Show and Tell: A Neural Image Caption Generator},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages     = {3156--3164},
    year      = {2015},
    publisher = {IEEE}
}

% Attention mechanism in captioning
@inproceedings{attention_xu2015,
    author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
    title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
    booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
    pages     = {2048--2057},
    year      = {2015}
}


% Transformer for captioning
@inproceedings{transformer_captioning_cornia2020,
    author    = {Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
    title     = {Meshed-Memory Transformer for Image Captioning},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages     = {10578--10587},
    year      = {2020},
    publisher = {IEEE}
}

% Accessibility applications
@inproceedings{accessibility_stangl2021,
    author    = {Stangl, Abigale and Bhowmick, Nitin and Kishore, Sai Sharath and Singhal, Kashish and Swaminathan, Ranjitha and Gleason, Cynthia and Morris, Meredith Ringel},
    title     = {Multi-Modal Mobility: An Image Description Dataset to Study the Role of Computer Vision in Accessible Image Captioning for People with Visual Impairments},
    booktitle = {ACM SIGACCESS Conference on Computers and Accessibility},
    year      = {2021},
    publisher = {ACM}
}

% Smartphone users statistics 2024
@misc{smartphone_users_2025,
    author       = {{Statista}},
    title        = {Number of smartphone mobile network subscriptions worldwide from 2016 to 2025},
    year         = {2025},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://www.statista.com/statistics/330695/number-of-smartphone-users-worldwide/}}
}

% Response times in HCI - Nielsen's classic work
@inbook{nielsen_response_times_1993,
    author    = {Jakob Nielsen},
    title     = {Usability Engineering},
    publisher = {Morgan Kaufmann},
    year      = {1993},
    chapter   = {5},
    pages     = {135--163}
}

% CLIP - Contrastive Language-Image Pre-training
@inproceedings{clip_radford2021,
    author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
    title     = {Learning Transferable Visual Models From Natural Language Supervision},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
    pages     = {8748--8763},
    year      = {2021},
    volume    = {139},
    series    = {Proceedings of Machine Learning Research},
    publisher = {PMLR},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2103.00020}}
}

% BLIP - Bootstrapping Language-Image Pre-training
@inproceedings{blip_li2022,
    author    = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
    title     = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
    booktitle = {Proceedings of the 39th International Conference on Machine Learning (ICML)},
    pages     = {12888--12900},
    year      = {2022},
    volume    = {162},
    series    = {Proceedings of Machine Learning Research},
    publisher = {PMLR},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2201.12086}}
}

% Florence - original
@inproceedings{florence_yuan2021,
    author    = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Lu, Pengchuan and Luo, Ruixiang and Song, Lijuan and Wang, Shaoqing and Wang, Yawen and Xie, Saining and Yang, Yu and Zhang, Lei and Zhang, Pengchuan and Zhou, Luowei and Zhu, Yue},
    title     = {Florence: A New Foundation Model for Computer Vision},
    journal   = {arXiv preprint arXiv:2111.11432},
    year      = {2021},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2111.11432}}
}

% Florence-2
@article{florence2_xiao2024,
    author  = {Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},
    title   = {Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks},
    journal = {arXiv preprint arXiv:2311.06242},
    year    = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2311.06242}}
}

% Flamingo
@article{flamingo_alayrac2022,
    author  = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
    title   = {Flamingo: a Visual Language Model for Few-Shot Learning},
    journal = {Advances in Neural Information Processing Systems (NeurIPS)},
    volume  = {35},
    pages   = {23716--23736},
    year    = {2022},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2204.14198}}
}

% GPT-2
@article{gpt2_radford2019,
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    title  = {Language Models are Unsupervised Multitask Learners},
    journal = {OpenAI Blog},
    year   = {2019},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}}
}

% GPT-4 Technical Report
@article{gpt4_openai2023,
    author = {{OpenAI}},
    title  = {GPT-4 Technical Report},
    journal = {arXiv preprint arXiv:2303.08774},
    year   = {2023},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2303.08774}}
}

% GPT-4V - The Dawn of LMMs
@article{gpt4v_yang2023,
    author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
    title  = {The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)},
    journal = {arXiv preprint arXiv:2309.17421},
    year   = {2023},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2309.17421}}
}

% GPT-4o
@misc{gpt4o_openai2024,
    author = {{OpenAI}},
    title  = {GPT-4o: Omni-modal AI Model},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/index/hello-gpt-4o/}}
}

% Gemini
@article{gemini_team2023,
    author = {{Gemini Team, Google}},
    title  = {Gemini: A Family of Highly Capable Multimodal Models},
    journal = {arXiv preprint arXiv:2312.11805},
    year   = {2023},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2312.11805}}
}

% Gemini 1.5
@article{gemini15_team2024,
    author = {{Gemini Team, Google}},
    title  = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
    journal = {arXiv preprint arXiv:2403.05530},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2403.05530}}
}

% Azure Computer Vision
@misc{azure_cv_microsoft2024,
    author = {{Microsoft Azure}},
    title  = {Azure Computer Vision API Documentation},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/}}
}

% ViT-GPT2 HuggingFace model
@misc{vitgpt2_nlpconnect,
    author = {{nlpconnect}},
    title  = {vit-gpt2-image-captioning},
    year   = {2022},
    howpublished = {HuggingFace Model Hub. Dostęp zdalny (2025-11-15): \url{https://huggingface.co/nlpconnect/vit-gpt2-image-captioning}}
}

@misc{vitgpt2_blog_kumar2022,
    title   = "The Illustrated Image Captioning using transformers",
    author  = "Kumar, Ankur",
    year    = "2022",
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/}}
}

% COCO Dataset
@inproceedings{coco_lin2014,
    author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
    title     = {Microsoft COCO: Common Objects in Context},
    booktitle = {European Conference on Computer Vision (ECCV)},
    pages     = {740--755},
    year      = {2014},
    publisher = {Springer},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/1405.0312}}
}

% DaViT - Dual Attention Vision Transformer
@inproceedings{davit_ding2022,
    author    = {Ding, Mingyu and Xiao, Bin and Codella, Noel and Luo, Ping and Wang, Jingdong and Yuan, Lu},
    title     = {DaViT: Dual Attention Vision Transformers},
    booktitle = {European Conference on Computer Vision (ECCV)},
    pages     = {74--92},
    year      = {2022},
    publisher = {Springer},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2204.03645}}
}

% BART - Denoising Sequence-to-Sequence Pre-training
@inproceedings{bart_lewis2020,
    author    = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
    title     = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
    pages     = {7871--7880},
    year      = {2020},
    publisher = {Association for Computational Linguistics},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/1910.13461}}
}

% ViT-GPT2 Blog/Tutorial reference
@misc{vitgpt2_blog_kumar2022,
    author = {Kumar, Nikhil},
    title  = {Image Captioning with ViT and GPT2},
    year   = {2022},
    howpublished = {HuggingFace Blog. Dostęp zdalny (2025-11-15): \url{https://huggingface.co/blog/image-captioning}}
}

% GPT-4o mini
@misc{gpt4omini_openai2024,
    author = {{OpenAI}},
    title  = {GPT-4o mini: advancing cost-efficient intelligence},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}
}

% OpenAI Pricing
@misc{openai_pricing2024,
    author = {{OpenAI}},
    title  = {OpenAI API Pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/api/pricing/}}
}

% Azure Computer Vision Documentation
@misc{azure_cv_docs2024,
    author = {{Microsoft Azure}},
    title  = {What is Azure AI Vision?},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview}}
}

% Azure Pricing
@misc{azure_pricing2024,
    author = {{Microsoft Azure}},
    title  = {Azure AI Vision pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/}}
}

% Gemini Team 2024
@article{gemini_team2024,
    author = {{Gemini Team, Google}},
    title  = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
    journal = {arXiv preprint arXiv:2403.05530},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2403.05530}}
}

% Gemini Pricing
@misc{gemini_pricing2024,
    author = {{Google Cloud}},
    title  = {Gemini API pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://ai.google.dev/pricing}}
}

% GPT-4o mini
@misc{gpt4omini_openai2024,
    author = {{OpenAI}},
    title  = {GPT-4o mini: advancing cost-efficient intelligence},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}
}

% OpenAI API Pricing
@misc{openai_pricing2024,
    author = {{OpenAI}},
    title  = {OpenAI API Pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/api/pricing/}}
}

% Azure Computer Vision Documentation
@misc{azure_cv_docs2024,
    author = {{Microsoft Azure}},
    title  = {What is Azure AI Vision?},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview}}
}

% Azure Pricing
@misc{azure_pricing2024,
    author = {{Microsoft Azure}},
    title  = {Azure AI Vision pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/}}
}

% Gemini Team 2024
@misc{gemini_team2024,
    author = {{Google DeepMind}},
    title  = {Gemini models},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://deepmind.google/technologies/gemini/}}
}

% Gemini 1.5 Report
@article{gemini_15_report2024,
    author = {{Gemini Team, Google}},
    title  = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
    journal = {arXiv preprint arXiv:2403.05530},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2403.05530}}
}

% Gemini Pricing
@misc{gemini_pricing2024,
    author = {{Google AI}},
    title  = {Gemini API Pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://ai.google.dev/pricing}}
}

