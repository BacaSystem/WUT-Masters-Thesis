% Artykuł w recenzowanym czasopiśmie.
@article{szczypiorski2015,
    author    = {Szczypiorski, K. and Janicki, A. and Wendzel, S},
    title     = {{T}he {G}ood, {T}he {B}ad {A}nd {T}he {U}gly: {E}valuation of {W}i-{F}i {S}teganography},
    journal   = {Journal of Communications},
    volume    = {10},
    number    = {10},
    pages     = {747--752},
    publisher = {Journal of Communications (JCM)},
    year      = {2015}
}

% Książka.
@book{goossens93,
    author    = {Michel Goossens and Frank Mittelbach and Alexander Samarin},
    title     = {The LaTeX Companion},
    publisher = {Addison-Wesley},
    address   = {Reading, Massachusetts},
    year      = {1993}
}

% Fragment książki (np. zakres stron).
@inbook{wang97,
    author    = {Hao Wang},
    title     = {A Logical Journey: From G{\"o}del to Philosophy.},
    publisher = {A Bradford Book},
    pages     = {316},
    year      = {1997}
}

% Fragment książki (np. esej), posiadający własny tytuł.
@incollection{goedel95,
    author    = {Kurt G{\"o}del},
    title     = {Texts relating to the ontological proof},
    booktitle = {Unpublished Essays and Lectures},
    publisher = {Oxford University Press},
    pages     = {429--437},
    year      = {1995}
}

% Publikacja konferencyjna.
@inproceedings{benzmuller2014,
    author       = {{Ch}. Benzmuller and B. W. Paleo},
    title        = {Automating {G\"o}del’s {O}ntological {P}roof of {G}od’s {E}xistence with {H}igher-order {A}utomated {T}heorem {P}rovers},
    booktitle    = {European	Conference on Artificial Intelligence},
    publisher    = {IOS Press},
    howpublished = {Dostęp zdalny (10.04.2019): \url{http://page.mi.fu-berlin.de/cbenzmueller/papers/C40.pdf}},
    year         = {2014}
}

% Raport techniczny.
@techreport{duqu2011,
    author      = {Bencsáth, B. and Pék, G. and Buttyán, L. and Félegyházi M.},
    title       = {{D}uqu: {A} {S}tuxnet-like malware found in the wild},
    institution = {Laboratory of Cryptography and System Security, Hungary},
    year        = {2011}
}

% Specyfikacja techniczna.
@manual{shs2015,
    title        = {{FIPS} 180-4: {S}ecure {H}ash {S}tandard ({SHS})},
    howpublished = {Dostęp zdalny (13.03.2019): \url{https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf}},
    year         = {2015}
}

% Praca magisterska.
@mastersthesis{wozniak2018,
    author = {Woźniak, Piotr},
    title  = {{P}rogramowanie kwadratowe w usuwaniu efektu rozmycia ruchu w fotografii cyfrowej},
    school = {Wydział Elektroniki i Technik Informacyjnych, Politechnika Warszawska},
    year   = {2018}
}

% Nieopublikowany artykuł, dostępny np. tylko w internecie.
@unpublished{koons2005,
    author = {Koons, Robert C.},
    title  = {{S}obel on {G\"o}del’s {O}ntological {P}roof},
    note   = {Dostęp zdalny (25.04.2019): \url{http://www.robkoons.net/media/69b0dd04a9d2fc6dffff80b4ffffd524.pdf}},
    year   = {2005}
}

% Źródło innego typu, np. repozytorium na GitHubie.
@misc{dcp19,
    author       = {Brodzki, Artur M.},
    title        = {{I}mplementation of own steganography protocol {DCP}-19, loosely based on {HICCUPS}},
    howpublished = {Dostęp zdalny (14.03.2019): \url{https://github.com/ArturB/DCP-19}},
    year         = {2019}
}



% -----------------------------------------------------
@misc {japan_ai_2016,
    author       = {{Ministry of Internal Affairs and Communications, Japan}},
    title        = {{White Paper on Information and Communications in Japan 2016}},
    howpublished = {Dostęp zdalny (2025-11-14): \url{https://www.soumu.go.jp/johotsusintokei/whitepaper/eng/WP2016/2016-index.html}},
    year         = {2016}
}

% Vision Transformer
@inproceedings{vit_dosovitskiy2021,
    author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
    title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    booktitle = {International Conference on Learning Representations (ICLR)},
    year      = {2021},
    howpublished = {Dostęp zdalny (2025-11-14): \url{https://arxiv.org/abs/2010.11929}}
}

% ONNX Runtime
@inproceedings{onnx_bai2019,
    author    = {Bai, Junjie and Lu, Fang and Zhang, Ke and others},
    title     = {{ONNX}: Open Neural Network Exchange},
    booktitle = {GitHub repository},
    year      = {2019},
    howpublished = {Dostęp zdalny (2025-11-14): \url{https://github.com/onnx/onnx}}
}
% ONNX (peer-reviewed reference discussing ONNX as a model exchange format)
@inproceedings{onnx_icsme_openja2022,
    author    = {Openja, Moses and Nikanjam, Amin and Haj Yahmed, Ahmed and Khomh, Foutse and Jiang, Zhen Ming},
    title     = {An Empirical Study of Challenges in Converting Deep Learning Models},
    booktitle = {2022 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
    year      = {2022},
    publisher = {IEEE},
    doi       = {10.1109/ICSME55016.2022.00032},
}

% TensorFlow Lite Quantization
@article{tflite_quantization_krishnamoorthi2018,
    author  = {Krishnamoorthi, Raghuraman},
    title   = {Quantizing deep convolutional networks for efficient inference: A whitepaper},
    journal = {arXiv preprint arXiv:1806.08342},
    year    = {2018},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/1806.08342}}
}

% TinyML - on-device machine learning
@article{tinyml_reddi2021,
    author  = {Reddi, Vijay Janapa and Plancher, Brian and Kennedy, Susan and Moroney, Laurence and Warden, Pete and Agarwal, Anant and Banbury, Colby and Banzi, Massimo and Bennett, Matthew and Carney, Nat and Chitnis, Nirvaana and Croce, Cristian and Kauffmann, Amy and Kiriu, Ian and Lin, Tiny Yiu-Leung and Patterso, Dale and Seshadri, Sandeep and Singh, Inderjit and Sordo, Jaime and Wilson, Evelyn},
    title   = {Widening Access to Applied Machine Learning with TinyML},
    journal = {arXiv preprint arXiv:2106.04008},
    year    = {2021},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2106.04008}}
}

% Edge Computing
@article{edge_computing_shi2016,
    author  = {Shi, Weisong and Cao, Jie and Zhang, Quan and Li, Youhuizi and Xu, Lanyu},
    title   = {Edge Computing: Vision and Challenges},
    journal = {IEEE Internet of Things Journal},
    volume  = {3},
    number  = {5},
    pages   = {637--646},
    year    = {2016},
    publisher = {IEEE}
}

% Edge vs Cloud comparison
@article{edge_vs_cloud_mao2017,
    author  = {Yuyi Mao and Changsheng You and Jun Zhang and Kaibin Huang and Khaled Letaief},
    title   = {A Survey on Mobile Edge Computing: The Communication Perspective},
    journal = {IEEE Communications Surveys \& Tutorials},
    volume  = {19},
    number  = {4},
    pages   = {2322--2358},
    year    = {2017},
    publisher = {IEEE}
}

% AI Market Precedence Research 2025
@misc{ai_market_precedence_2025,
    author       = {{Precedence Research}},
    title        = {Artificial Intelligence (AI) Market Size, Share and Trends 2025 to 2034},
    year         = {2025},
    note         = {Report Code: 1635},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://www.precedenceresearch.com/artificial-intelligence-market}}
}

% Image Captioning Survey
@article{image_captioning_survey_hossain2019,
    author  = {Hossain, Md Zakir and Sohel, Ferdous and Shiratuddin, Mohd Fairuz and Laga, Hamid},
    title   = {A Comprehensive Survey of Deep Learning for Image Captioning},
    journal = {ACM Computing Surveys},
    volume  = {51},
    number  = {6},
    pages   = {1--36},
    year    = {2019},
    publisher = {ACM}
}

% Show and Tell
@inproceedings{show_and_tell_vinyals2015,
    author    = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
    title     = {Show and Tell: A Neural Image Caption Generator},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages     = {3156--3164},
    year      = {2015},
    publisher = {IEEE}
}

% Attention mechanism in captioning
@inproceedings{attention_xu2015,
    author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
    title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
    booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
    pages     = {2048--2057},
    year      = {2015}
}


% Transformer for captioning
@inproceedings{transformer_captioning_cornia2020,
    author    = {Cornia, Marcella and Stefanini, Matteo and Baraldi, Lorenzo and Cucchiara, Rita},
    title     = {Meshed-Memory Transformer for Image Captioning},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages     = {10578--10587},
    year      = {2020},
    publisher = {IEEE}
}

% Accessibility applications
@inproceedings{accessibility_stangl2021,
    author    = {Stangl, Abigale and Bhowmick, Nitin and Kishore, Sai Sharath and Singhal, Kashish and Swaminathan, Ranjitha and Gleason, Cynthia and Morris, Meredith Ringel},
    title     = {Multi-Modal Mobility: An Image Description Dataset to Study the Role of Computer Vision in Accessible Image Captioning for People with Visual Impairments},
    booktitle = {ACM SIGACCESS Conference on Computers and Accessibility},
    year      = {2021},
    publisher = {ACM}
}

% Smartphone users statistics 2024
@misc{smartphone_users_2025,
    author       = {{Statista}},
    title        = {Number of smartphone mobile network subscriptions worldwide from 2016 to 2025},
    year         = {2025},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://www.statista.com/statistics/330695/number-of-smartphone-users-worldwide/}}
}

% Response times in HCI - Nielsen's classic work
@inbook{nielsen_response_times_1993,
    author    = {Jakob Nielsen},
    title     = {Usability Engineering},
    publisher = {Morgan Kaufmann},
    year      = {1993},
    chapter   = {5},
    pages     = {135--163}
}

% CLIP - Contrastive Language-Image Pre-training
@inproceedings{clip_radford2021,
    author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
    title     = {Learning Transferable Visual Models From Natural Language Supervision},
    booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
    pages     = {8748--8763},
    year      = {2021},
    volume    = {139},
    series    = {Proceedings of Machine Learning Research},
    publisher = {PMLR},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2103.00020}}
}

% BLIP - Bootstrapping Language-Image Pre-training
@inproceedings{blip_li2022,
    author    = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
    title     = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
    booktitle = {Proceedings of the 39th International Conference on Machine Learning (ICML)},
    pages     = {12888--12900},
    year      = {2022},
    volume    = {162},
    series    = {Proceedings of Machine Learning Research},
    publisher = {PMLR},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2201.12086}}
}

% Florence - original
@inproceedings{florence_yuan2021,
    author    = {Yuan, Lu and Chen, Dongdong and Chen, Yi-Ling and Codella, Noel and Dai, Xiyang and Gao, Jianfeng and Hu, Houdong and Huang, Xuedong and Li, Boxin and Li, Chunyuan and Liu, Ce and Lu, Pengchuan and Luo, Ruixiang and Song, Lijuan and Wang, Shaoqing and Wang, Yawen and Xie, Saining and Yang, Yu and Zhang, Lei and Zhang, Pengchuan and Zhou, Luowei and Zhu, Yue},
    title     = {Florence: A New Foundation Model for Computer Vision},
    journal   = {arXiv preprint arXiv:2111.11432},
    year      = {2021},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2111.11432}}
}

% Florence-2
@article{florence2_xiao2024,
    author  = {Xiao, Bin and Wu, Haiping and Xu, Weijian and Dai, Xiyang and Hu, Houdong and Lu, Yumao and Zeng, Michael and Liu, Ce and Yuan, Lu},
    title   = {Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks},
    journal = {arXiv preprint arXiv:2311.06242},
    year    = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2311.06242}}
}

% Flamingo
@article{flamingo_alayrac2022,
    author  = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
    title   = {Flamingo: a Visual Language Model for Few-Shot Learning},
    journal = {Advances in Neural Information Processing Systems (NeurIPS)},
    volume  = {35},
    pages   = {23716--23736},
    year    = {2022},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2204.14198}}
}

% GPT-2
@article{gpt2_radford2019,
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    title  = {Language Models are Unsupervised Multitask Learners},
    journal = {OpenAI Blog},
    year   = {2019},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}}
}

% GPT-4 Technical Report
@article{gpt4_openai2023,
    author = {{OpenAI}},
    title  = {GPT-4 Technical Report},
    journal = {arXiv preprint arXiv:2303.08774},
    year   = {2023},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2303.08774}}
}

% GPT-4V - The Dawn of LMMs
@article{gpt4v_yang2023,
    author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
    title  = {The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)},
    journal = {arXiv preprint arXiv:2309.17421},
    year   = {2023},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2309.17421}}
}

% GPT-4o
@misc{gpt4o_openai2024,
    author = {{OpenAI}},
    title  = {GPT-4o: Omni-modal AI Model},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/index/hello-gpt-4o/}}
}

% Gemini
@article{gemini_team2023,
    author = {{Gemini Team, Google}},
    title  = {Gemini: A Family of Highly Capable Multimodal Models},
    journal = {arXiv preprint arXiv:2312.11805},
    year   = {2023},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2312.11805}}
}

% Gemini 1.5
@article{gemini15_team2024,
    author = {{Gemini Team, Google}},
    title  = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
    journal = {arXiv preprint arXiv:2403.05530},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2403.05530}}
}

% Azure Computer Vision
@misc{azure_cv_microsoft2024,
    author = {{Microsoft Azure}},
    title  = {Azure Computer Vision API Documentation},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/}}
}

% ViT-GPT2 HuggingFace model
@misc{vitgpt2_nlpconnect,
    author = {{nlpconnect}},
    title  = {vit-gpt2-image-captioning},
    year   = {2022},
    howpublished = {HuggingFace Model Hub. Dostęp zdalny (2025-11-15): \url{https://huggingface.co/nlpconnect/vit-gpt2-image-captioning}}
}

@misc{vitgpt2_blog_kumar2022,
    title   = "The Illustrated Image Captioning using transformers",
    author  = "Kumar, Ankur",
    year    = "2022",
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/}}
}

% COCO Dataset
@article{coco_lin2014,
      author       = {Tsung{-}Yi Lin and
                  Michael Maire and
                  Serge J. Belongie and
                  Lubomir D. Bourdev and
                  Ross B. Girshick and
                  James Hays and
                  Pietro Perona and
                  Deva Ramanan and
                  Piotr Doll{\'{a}}r and
                  C. Lawrence Zitnick},
  title        = {Microsoft {COCO:} Common Objects in Context},
  journal      = {CoRR},
  volume       = {abs/1405.0312},
  year         = {2014},
  url          = {http://arxiv.org/abs/1405.0312},
  eprinttype    = {arXiv},
  eprint       = {1405.0312},
  timestamp    = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

% DaViT - Dual Attention Vision Transformer
@inproceedings{davit_ding2022,
    author    = {Ding, Mingyu and Xiao, Bin and Codella, Noel and Luo, Ping and Wang, Jingdong and Yuan, Lu},
    title     = {DaViT: Dual Attention Vision Transformers},
    booktitle = {European Conference on Computer Vision (ECCV)},
    pages     = {74--92},
    year      = {2022},
    publisher = {Springer},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2204.03645}}
}

% BART - Denoising Sequence-to-Sequence Pre-training
@inproceedings{bart_lewis2020,
    author    = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
    title     = {BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
    pages     = {7871--7880},
    year      = {2020},
    publisher = {Association for Computational Linguistics},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/1910.13461}}
}

% ViT-GPT2 Blog/Tutorial reference
@misc{vitgpt2_blog_kumar2022,
    author = {Kumar, Nikhil},
    title  = {Image Captioning with ViT and GPT2},
    year   = {2022},
    howpublished = {HuggingFace Blog. Dostęp zdalny (2025-11-15): \url{https://huggingface.co/blog/image-captioning}}
}

% GPT-4o mini
@misc{gpt4omini_openai2024,
    author = {{OpenAI}},
    title  = {GPT-4o mini: advancing cost-efficient intelligence},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}
}

% OpenAI Pricing
@misc{openai_pricing2024,
    author = {{OpenAI}},
    title  = {OpenAI API Pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/api/pricing/}}
}

% Azure Computer Vision Documentation
@misc{azure_cv_docs2024,
    author = {{Microsoft Azure}},
    title  = {What is Azure AI Vision?},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview}}
}

% Azure Pricing
@misc{azure_pricing2024,
    author = {{Microsoft Azure}},
    title  = {Azure AI Vision pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/}}
}

% Gemini Team 2024
@article{gemini_team2024,
    author = {{Gemini Team, Google}},
    title  = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
    journal = {arXiv preprint arXiv:2403.05530},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2403.05530}}
}

% Gemini Pricing
@misc{gemini_pricing2024,
    author = {{Google Cloud}},
    title  = {Gemini API pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://ai.google.dev/pricing}}
}

% GPT-4o mini
@misc{gpt4omini_openai2024,
    author = {{OpenAI}},
    title  = {GPT-4o mini: advancing cost-efficient intelligence},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}
}

% OpenAI API Pricing
@misc{openai_pricing2024,
    author = {{OpenAI}},
    title  = {OpenAI API Pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://openai.com/api/pricing/}}
}

% Azure Computer Vision Documentation
@misc{azure_cv_docs2024,
    author = {{Microsoft Azure}},
    title  = {What is Azure AI Vision?},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview}}
}

% Azure Pricing
@misc{azure_pricing2024,
    author = {{Microsoft Azure}},
    title  = {Azure AI Vision pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://azure.microsoft.com/en-us/pricing/details/cognitive-services/computer-vision/}}
}

% Gemini Team 2024
@misc{gemini_team2024,
    author = {{Google DeepMind}},
    title  = {Gemini models},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://deepmind.google/technologies/gemini/}}
}

% Gemini 1.5 Report
@article{gemini_15_report2024,
    author = {{Gemini Team, Google}},
    title  = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
    journal = {arXiv preprint arXiv:2403.05530},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://arxiv.org/abs/2403.05530}}
}

% Gemini Pricing
@misc{gemini_pricing2024,
    author = {{Google AI}},
    title  = {Gemini API Pricing},
    year   = {2024},
    howpublished = {Dostęp zdalny (2025-11-15): \url{https://ai.google.dev/pricing}}
}

% CIDEr metric for image captioning
@inproceedings{cider_vedantam2015,
    author    = {Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
    title     = {CIDEr: Consensus-based Image Description Evaluation},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages     = {4566--4575},
    year      = {2015},
    publisher = {IEEE},
    howpublished = {Dostęp zdalny (2025-11-16): \url{https://arxiv.org/abs/1411.5726}}
}

% SPICE metric for image captioning
@inproceedings{spice_anderson2016,
    author    = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
    title     = {SPICE: Semantic Propositional Image Caption Evaluation},
    booktitle = {European Conference on Computer Vision (ECCV)},
    pages     = {382--398},
    year      = {2016},
    publisher = {Springer},
    howpublished = {Dostęp zdalny (2025-11-16): \url{https://arxiv.org/abs/1607.08822}}
}

% METEOR metric
@inproceedings{meteor_banerjee2005,
    author    = {Banerjee, Satanjeev and Lavie, Alon},
    title     = {METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments},
    booktitle = {Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
    pages     = {65--72},
    year      = {2005},
    publisher = {Association for Computational Linguistics},
    howpublished = {Dostęp zdalny (2025-11-16): \url{https://aclanthology.org/W05-0909/}}
}

% BLEU metric
@inproceedings{bleu_papineni2002,
    author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
    title     = {BLEU: a Method for Automatic Evaluation of Machine Translation},
    booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)},
    pages     = {311--318},
    year      = {2002},
    publisher = {Association for Computational Linguistics},
    howpublished = {Dostęp zdalny (2025-11-16): \url{https://aclanthology.org/P02-1040/}}
}

% BLEU validity review
@article{bleu_validity_reiter2018,
    author = {Reiter, Ehud},
    title = {A Structured Review of the Validity of BLEU},
    journal = {Computational Linguistics},
    volume = {44},
    number = {3},
    pages = {393-401},
    year = {2018}
}



%Strategy pattern
@incollection{strategy_pattern_schmidt,
    title = {Architectures and Patterns for Developing High-performance, Real-time ORB Endsystems},
    author = {Douglas C. Schmidt and David L. Levine and Chris Cleeland},
    series = {Advances in Computers},
    publisher = {Elsevier},
    volume = {48},
    pages = {1-118},
    year = {1999},
    booktitle = {Distributed Information Resources},
}

% OpenAI Chat Completions
@misc{openai_completions2025,
    author = {{OpenAI}},
    title  = {OpenAI Docs Images and vision},
    year   = {2025},
    howpublished = {Dostęp zdalny (2026-01-02): \url{https://platform.openai.com/docs/guides/images-vision?api-mode=chat&format=base64-encoded}}
}

% Hallucinations in AI mobile apps
@article{massenon2025,
    author  = {Massenon, R. and Gambo, I. and Khan, J.A. and others},
    title   = {"My AI is Lying to Me": User-reported LLM hallucinations in AI mobile apps reviews},
    journal = {Scientific Reports},
    volume  = {15},
    pages   = {30397},
    year    = {2025},
    doi     = {10.1038/s41598-025-15416-8},
    howpublished = {Dostęp zdalny (2026-01-02): \url{https://doi.org/10.1038/s41598-025-15416-8}}
}

% AzureCV Florence based
@misc{azurecv_florence,
  author = {{Microsoft Azure}},
  howpublished = {Dostęp zdalny (2026-01-02): \url{https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/concept-describe-images-40?tabs=image}},
  title = {Azure Computer Vision - Image Captions (version 4.0)},
  year = {2025}
}

% Transformer Architecture 
@article{transformer_vaswani2017,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% Domain Adaptation
@article{domain_adaptation_pan2009,
    author = {Pan, Sinno Jialin and Yang, Qiang},
    title = {A Survey on Transfer Learning},
    journal = {IEEE Transactions on Knowledge and Data Engineering},
    volume = {22},
    number = {10},
    pages = {1345--1359},
    year = {2010},
    publisher = {IEEE},
    doi = {10.1109/TKDE.2009.191},
}

% Transfer Learning Limitations 
@inproceedings{transfer_learning_limitations_kenton2019,
    author = {Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Kristina},
    title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages = {4171--4186},
    year = {2019},
    publisher = {Association for Computational Linguistics},
    doi = "10.18653/v1/N19-1423",
}

% Hallucination in Large Language Models 
@article{hallucination_ji2023,
    author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
    title = {Survey of Hallucination in Natural Language Generation},
    year = {2023},
    publisher = {Association for Computing Machinery},
    volume = {55},
    number = {12},
    doi = {10.1145/3571730},
    journal = {ACM Comput. Surv.},
    pages = {1--38},
}

% Android Power Profiling
@article{android_power_profiling,
    author = {Pathak, Abhinav and Hu, Y. Charlie and Zhang, Ming},
    title = {Where is the energy spent inside my app? Fine Grained Energy Accounting on Smartphones with Eprof},
    booktitle = {Proceedings of the 7th ACM European Conference on Computer Systems},
    pages = {29--42},
    year = {2012},
    publisher = {ACM},
    doi = {10.1145/2168836.2168841},
}

% Mobile Energy Consumption
@article{mobile_energy_pathak2012,
    author = {Pathak, Abhinav and Hu, Y. Charlie and Zhang, Ming and Bahl, Paramvir and Wang, Yi-Min},
    title = {Fine-grained power modeling for smartphones using system call tracing},
    booktitle = {Proceedings of the sixth conference on Computer systems},
    pages = {153--168},
    year = {2011},
    publisher = {ACM},
    doi = {10.1145/1966445.1966460},
}

% Human vs Automatic Evaluation
@article{human_evaluation_necessity_belz2009,
    author = {Shutova, Ekaterina and Sun, Lin and Gutiérrez, Elkin Darío and Lichtenstein, Patricia and Narayanan, Srini},
    title = {Multilingual Metaphor Processing: Experiments with Semi-Supervised and Unsupervised Learning},
    journal = {Computational Linguistics},
    volume = {43},
    number = {1},
    pages = {71-213},
    year = {2017},
    doi = {10.1162/coli.2009.35.4.35405},
}

% Thermal Throttling
@article{panchal2024thermal,
  title={Thermal and power management challenges in high-performance mobile processors},
  author={Panchal, Vinay},
  journal={International Journal of Innovative Research of Science, Engineering and Technology (IJIRSET)},
  volume={13},
  number={11},
  year={2024}
}
