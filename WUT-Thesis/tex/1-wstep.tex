\newpage % Rozdziały zaczynamy od nowej strony.
\section*{Cel Pracy} \label{ss:Cel Pracy}

\noindent
Dynamiczny rozwój technologii oraz systemów sztucznej inteligencji umożliwił
realizację zaawansowanych i skomplikowanych obliczeniowo zadań bezpośrednio na urządzeniach mobilnych.
Postęp w architekturze modeli, metodach kompresji oraz wydajności układów (CPU/NPU) sprawił,
że aplikacje korzystające z AI stały się powszechne w środowisku mobilnym.
Równolegle, wielcy dostawcy usług chmurowych udostępniają najnowocześniejsze modele AI,
oferujące zaawansowane możliwości o niespotykanej wcześniej jakości i skali.
Rozwiązania te niosą jednak ze sobą zależność od zewnętrznej infrastruktury oraz wiążą się
z kwestiami prywatności, bezpieczeństwa i opłat subskrypcyjnych.

Na tym tle szczególne znaczenie zyskuje zadanie automatycznego generowania opisów obrazów 
(image captioning), łączące zaawansowane przetwarzanie obrazu z generowaniem języka naturalnego.
Technologia ta znajduje zastosowanie w wielu aspektach, od wsparcia osób z dysfunkcją wzroku,
przez automatyczną kategoryzację treści, aż po integrację z asystentami głosowymi.


W obliczu tych możliwości twórcy aplikacji mobilnych stają przed dylematem wyboru
między wdrażaniem lokalnych modeli sztucznej inteligencji, a integracją z usługami chmurowych dostawców.

Celem niniejszej pracy jest przeprowadzenie kompleksowej analizy porównawczej modeli działających 
w warunkach lokalnych oraz chmurowych w kontekście generowania opisów obrazów na urządzeniach 
z systemem Android. Badanie obejmuje porównanie trzech modeli lokalnych uruchamianych przez środowisko
ONNX Runtime (Florence-2, ViT-GPT2, BLIP) oraz trzech usług chmurowych udostępnianych przez interfejs
API (OpenAI GPT-4o mini, Azure Computer Vision, Google Gemini 2.5 Flash Lite).

Analiza efektywności poszczególnych modeli opiera się na dwóch komplementarnych aspektach:
metrykach wydajnościowych (czas generowania odpowiedzi, zużycie pamięci RAM, pobór energii, koszty wywołań API)
oraz metrykach jakościowych (poprawność generowanych opisów mierzona wskaźnikami CIDEr, SPICE, METEOR, BLEU).
Zebrane w ten sposób dane posłużą do sformułowania popartych empirycznie rekomendacji doboru rozwiązań
sztucznej inteligencji w zależności od przeznaczenia projektowanej aplikacji mobilnej.

Do realizacji powyższych celów zaprojektowano i zaimplementowano platformę badawczą CaptionLab, która
umożliwia prowadzenie badań, zbieranie metryk oraz zautomatyzowane testy na dużych zbiorach danych.
Architektura platformy badawczej została opracowana z myślą o elastyczności i rozszerzalności,
dzięki jednolitemu podejściu do implementacji poszczególnych modeli w formie osobnych providerów.

Ostatecznie praca dostarcza nie tylko analityczną wiedzę odnośnie strategii wdrażania systemów 
sztucznej inteligencji w środowisku mobilnym, ale także funkcjonalną platformę badawczą, która
może służyć jako fundament dla dalszych badań lub jako funkcjonalne narzędzie przy podejmowaniu
decyzji architektonicznych w projektowaniu aplikacji mobilnych.



\newpage
\section{Wstęp}\label{s:Wstep}

\noindent
Systemy sztucznej inteligencji w ostatnich latach przeszły znaczną ewolucję od naukowych koncepcji
do powszechnie wykorzystywanych narzędzi, które zrewolucjonizowały wiele dziedzin życia i przemysłu.
AI znalazła zastosowanie i zmieniła funkcjonowanie całych sektorów gospodarki, począwszy od technologii i
rozrywki przez dziennikarstwo, kończąc na finansach, czy medycynie. Według danych Presedence Research globalny rynek AI
osiągnął wartość ponad 750 miliardów USD w 2025 roku, a prognozowany dalszy wzrost, przewiduje wzrost
do prawie 3.7 bilionów USD do roku 2034 \cite{ai_market_precedence_2025}.

W spektrum zastosowań sztucznej inteligencji szczególnie wyróżnia się zadanie automatycznego generowania
opisów obrazów (image captioning) jako jedno z bardziej złożonych wyzwań łączących przetwarzanie obrazu,
kategoryzację wizualną oraz analizę kontekstu z generowaniem języka naturalnego. Systemy image captioning,
oparte na architekturach encoder-decoder \cite{show_and_tell_vinyals2015}, znajdują zastosowanie w wielu obszarach,
od wsparcia osób z dysfunkcją wzroku, przez automatyczną kategoryzację treści, aż po integrację
z asystentami głosowymi.

Rozwój technologii uczenia maszynowego umożliwił znaczący postęp w dziedzinie image captioning.
Systemy AI zyskały nowe możliwości dzięki innowacyjnym architekturom modeli oraz narzędziom
ułatwiającym deweloperom wdrażanie zaawansowanych rozwiązań.
Modele oparte na architekturze transformerów wizyjnych (Vision Transformer, ViT)
wypierały tradycyjne sieci konwolucyjne (Convolutional Neural Networks, CNN), oferując lepsze możliwości 
rozpoznawania i kwalifikacji obrazów \cite{vit_dosovitskiy2021}. Pojawienie się technologii jednolitych 
formatów modeli takich jak ONNX (Open Neural Network Exchange) umożliwiło łatwiejszą wymianę i wdrażanie 
zaawansowanych modeli AI w środowiskach o ograniczonych zasobach \cite{onnx_icsme_openja2022}, podczas gdy 
popularyzacja sieci 5G oraz rozwój chmurowego przetwarzania danych pozwoliło na korzystanie z potężnych
modeli AI poprzez interfejsy API (Application Programming Interface), eliminując potrzebę lokalnego przechowywania
i przetwarzania danych \cite{edge_computing_shi2016}.

Jednym z najbardziej dynamicznie rozwijających się obszarów zastosowań sztucznej inteligencji
są aplikacje i systemy mobilne. Współczesne urządzenia mobilne, niegdyś pełniące funkcję jedynie
narzędzi komunikacji i prostej rozrywki, dziś wyposażone są w zaawansowane układy obliczeniowe
(Neural Processing Unit, NPU) umożliwiające przetwarzanie złożonych modeli AI bezpośrednio na urządzeniach. Według raportu z 2025 roku opublikowanego przez Statista, na świecie jest ponad 7 miliardów
zarejestrowanych w sieci smartfonów, co przekłada się na niemal 80\% globalnej populacji \cite{smartphone_users_2025}.
Ta masywna baza użytkowników połączona z ciągle rosnącą mocą obliczeniową układów mobilnych, sprawia,
że platformy mobilne stają się kluczowym obszarem dla wdrażania rozwiązań opartych na sztucznej inteligencji.


Wybór aplikacji mobilnych jako obszaru badań nad efektywnością modeli AI jest konsekwencją szeregu specyficznych uwarunkowań środowiskowych.
Po pierwsze, urządzenia mobilne charakteryzują się ścisłymi ograniczeniami sprzętowymi, takimi jak limitowana ilość pamięci RAM, mocy obliczeniowej, 
czy ograniczonym źródłem zasilania. Po drugie, urządzenia funkcjonują w warunkach zmiennej jakości połączeń sieciowych, co podważa niezawodność rozwiązań 
zależnych wyłącznie od infrastruktury chmurowych dostawców. Po trzecie, kwestia prywatności i bezpieczeństwa danych użytkownika nabiera szczególnego 
znaczenia w kontekście osobistych urządzeń z dostępem do bardzo wrażliwych danych. Czynniki te powodują, że architektura systemów AI na platformach mobilnych
musi uwzględniać precyzyjny dobór rozwiązań, miejsca przetwarzania danych oraz strategii zarządzania zasobami.

W kontekście tych ograniczeń zarysowuje się istotny kompromis między podejściem lokalnym a chmurowym.
Modele uruchamiane lokalnie oferują działanie bez połączenia z siecią,
przewidywalność czasu odpowiedzi oraz silniejszą ochronę prywatności. Z drugiej strony usługi chmurowe oferują dostęp 
do najnowszych, dużych generatywnych modeli bez konieczności ich instalacji, aktualizacji, czy obciążania pamięci urządzenia,
kosztem zależności od sieci, kosztów subskrypcyjnych i zewnętrznego przetwarzania danych. Badania Yuyi Mao i innych wykazały,
że wybór miejsca wykonywania obliczeń (edge vs. cloud) ma istotny wpływ na opóźnienie, zużycie energii
oraz przepustowość systemu \cite{edge_vs_cloud_mao2017}. W przypadku środowiska o ograniczonych zasobach, 
chcąc jednocześnie oferować możliwie jak najlepszą jakość usługi, wybór odpowiedniego rozwiązania staje się kluczowym zadaniem projektowym dla twórców aplikacji mobilnych.


Brak kompleksowych badań porównawczych utrudnia jednak podejmowanie świadomych decyzji architektonicznych.
Niniejsza praca ma na celu wypełnienie tej luki poprzez przeprowadzenie szczegółowej analizy wydajnościowej
i jakościowej reprezentatywnych modeli lokalnych i chmurowych do generowania opisów obrazów w środowisku
mobilnym Android.


\subsection{Struktura Pracy}\label{ss:Struktura Pracy}

\noindent
Przeprowadzenie analizy wymagało w pierwszej kolejności zebrania dogłębnej wiedzy teoretycznej na temat image captioning'u
oraz architektury poszczególnych rozwiązań, aby zdefiniować cel i zakres badań.
Szczegółowy opis teoretycznych podstaw automatycznego generowania opisów obrazów, ewolucji architektur
oraz mechanizmów działania systemów image captioning przedstawiono w rozdziale \ref{s:Modele}. \textit{Modele i technologie AI do generowania opisów obrazów}.
Sekcja \ref{ss:Wymagania Projektowe}. \textit{Wymagania projektowe} opisuje wymagania projektowe aplikacji badawczej.
Pogłębiona analiza i dobór odpowiednich modeli do generowania opisów zostały ujęte w ramach tego samego rozdziału.

W ramach pracy napisany został dedykowany system badawczy CaptionLab, zdolny do przeprowadzania
testów porównawczych między różnymi modelami AI w środowisku Android, jednocześnie zbierając potrzebne 
metryki do dalszej analizy. Szczegóły architektury i implementacji platformy badawczej zostały opisane
w rozdziale \ref{s:Aplikacja badawcza CaptionLab}. \textit{Aplikacja badawcza CaptionLab}.

W rozdziale \ref{s:Metodologia badan}. \textit{Metodologia badania} przedstawiono proces projektowania i przeprowadzania 
eksperymentów badawczych, metodologię pomiarów, a także opis wykorzystanych w analizie metryk.


Wszelkie dane zebrane podczas przeprowadzania badań zostały przedstawione w rozdziale \ref{s:Wyniki badan}. \textit{Wyniki badań},
aby na ich podstawie dokonać analizy i wysnuć odpowiednią interpretację na łamach rozdziału \ref{s:Interpretacja wynikow badan}. \textit{Interpretacja wyników badań}.
Rozdział \ref{s:Podsumowanie}. \textit{Podsumowanie} zawiera końcowe wnioski z przeprowadzonych badań oraz wskazuje kierunki dalszych badań w obszarze wdrażania systemów
image captioning'u  opartych o AI na platformach mobilnych.

\subsection{Wymagania projektowe}\label{ss:Wymagania Projektowe}
\noindent
W celu efektywnego przeprowadzenia badań porównawczych modeli lokalnych i chmurowych zdefiniowano następujące wymagania projektowe dla aplikacji badawczej CaptionLab:

\textbf{Wymagania funkcjonalne:}
\begin{enumerate}
\item Wsparcie dla minimum 3 modeli lokalnych (Florence-2, ViT-GPT2, BLIP)
\item Wsparcie dla minimum 3 modeli chmurowych (OpenAI, Azure, Gemini)
\item Możliwość testowania pojedynczych obrazów z interfejsem użytkownika
\item Możliwość automatycznych testów batchowych na zestawach obrazów
\item Zbieranie metryk: czas (preprocessing, inference, postprocessing), pamięć, energia, koszty
\item Eksport wyników do formatów CSV i JSON
\end{enumerate}

\textbf{Wymagania niefunkcjonalne:}
\begin{enumerate}
\item Platforma: Android SDK 26+ (Android 8.0 Oreo i nowsze)
\item Architektura: modularna, łatwo rozszerzalna o nowe modele
\item Wydajność: minimalne overhead pomiarowe (<5\% czasu inference)
\item Dokładność pomiarów: precision ±50ms dla czasu, ±10MB dla pamięci
\item Kod: Kotlin, zgodny z Android best practices
\end{enumerate}
