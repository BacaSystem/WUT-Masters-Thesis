% \newpage % Rozdziały zaczynamy od nowej strony.
\clearpage
\section{Aplikacja badawcza CaptionLab}\label{s:Aplikacja badawcza CaptionLab}
\noindent
W celu przeprowadzenia kompleksowych badań porównawczych wybranych modeli AI, zaprojektowano 
dedykowaną aplikację badawczą \textquote{CaptionLab} na system Android. Platforma ta, musiała umożliwiać testowanie
zarówno lokalnych oraz chmurowych modeli AI,
dostarczać odpowiednich narzędzi do zbierania danych w trakcie inferencji, a także oferować możliwość eksportu zebranych danych do dalszej analizy.

W tym rozdziale opisano architekturę systemu badawczego, jego kluczowe komponenty odpowiedzialne za generowanie opisów obrazów 
i pomiar parametrów wydajności, a także mechanizm dostarczania modeli AI, system zbierania metryk oraz moduł testów automatycznych.

\subsection{Architektura systemu}\label{ss:Architektura systemu}
\noindent
Budowa platformy \textquote{CaptionLab} opiera się na modularnej architekturze, złożonej z wyraźnie wydzielonych warstw funkcjonalnych.
System został podzielony na trzy główne warstwy: warstwę prezentacji odpowiedzialną za interfejs i interakcję z użytkownikiem,
warstwę logiki biznesowej odpowiedzialną za przeprowadzanie testów i przepływ danych oraz warstwę providerów AI implementującą 
konkretne modele sztucznej inteligencji.

Schemat architektury wysokiego poziomu przedstawiono na rysunku \ref{fig:captionlab_architecture}., który ilustruje 
najważniejsze elementy platformy, wzajemne relacje między poszczególnymi warstwami oraz przepływ danych w systemie.


\begin{figure}[!h]
    \centering
    \begin{tikzpicture}[
        node distance=0.6cm,
        layer/.style={rectangle, draw=black, very thick, minimum height=9.5cm, align=center},
        component/.style={rectangle, draw, thick, rounded corners=2pt, minimum width=2.2cm, minimum height=0.8cm, align=center, font=\footnotesize},
        widecomp/.style={rectangle, draw, thick, rounded corners=2pt, minimum width=3.8cm, minimum height=1.5cm, align=center, font=\small},
        smallcomp/.style={rectangle, draw, thick, rounded corners=2pt, minimum width=1.8cm, minimum height=0.65cm, align=center, font=\scriptsize},
        interface/.style={rectangle, draw, thick, dashed, minimum height=9cm, minimum width=0.8cm, align=center},
        flow/.style={->, thick, >=stealth},
        dataflow/.style={->, thick, >=stealth, dashed}
    ]
        % WARSTWA PREZENTACJI 
        \node[layer, fill=blue!10, minimum width=2.8cm] (pres_layer) at (0,0) {};
        \node[above=0.05cm of pres_layer.north, font=\bfseries\scriptsize, align=center] {WARSTWA\\PREZENTACJI};
        
        \node[component, fill=blue!25] (main_act) at (0,2.2) {Main\\Activity\\{\tiny Single Test}};
        \node[component, fill=blue!25] (batch_act) at (0,-2.2) {BatchTest\\Activity\\{\tiny Batch Tests}};
        
        % WARSTWA LOGIKI BIZNESOWEJ
        \node[layer, fill=orange!10, minimum width=5.5cm, right=0.8cm of pres_layer] (logic_layer) {};
        \node[above=0.05cm of logic_layer.north, font=\bfseries\scriptsize, align=center] {WARSTWA\\LOGIKI BIZNESOWEJ};
        
        \node[widecomp, fill=orange!30] (provider_mgr) at ($(logic_layer)+(0,0)$) {Provider Manager\\{\tiny Models Coordination}};
        \node[component, fill=orange!25] (metrics) at ($(logic_layer)+(0,2.2)$) {Metrics\\Collector\\{\tiny Measurements}};

        \node[component, fill=orange!25] (benchmark) at ($(logic_layer)+(0,-2.2)$) {Benchmark\\Runner\\{\tiny Orchestration}};
        
        \node[smallcomp, fill=yellow!20] (memory_mon) at ($(metrics)+(-1.1,1.8)$) {Memory\\Monitor};
        \node[smallcomp, fill=yellow!20] (power_mon) at ($(metrics)+(1.1,1.8)$) {Power\\Monitor};
        
        \node[smallcomp, fill=yellow!20] (exporter) at ($(benchmark)+(0,-1.8)$) {Data\\Exporter};
        
        % WARSTWA PROVIDERÓW 
        \node[layer, fill=green!8, minimum width=4.5cm, right=0.8cm of logic_layer] (provider_layer) {};
        \node[above=0.05cm of provider_layer.north, font=\bfseries\scriptsize, align=center] {WARSTWA\\PROVIDERÓW AI};
        
        % INTERFACE
        \node[interface, fill=purple!8] (interface_layer) at ($(provider_layer.west)+(0.75,0)$) {};
        \node[font=\bfseries\scriptsize, rotate=90] at (interface_layer) {Captioning Provider Interface};
        
        % Providery - lokalne
        \node[smallcomp, fill=cyan!25] (florence) at ($(provider_layer)+(1,3.3)$) {Florence-2\\{\tiny ONNX}};
        \node[smallcomp, fill=cyan!25] (vitgpt2) at ($(provider_layer)+(1,2.1)$) {ViT-GPT2\\{\tiny ONNX}};
        \node[smallcomp, fill=cyan!25] (blip) at ($(provider_layer)+(1,0.9)$) {BLIP\\{\tiny ONNX}};
        
        % Providery - chmurowe
        \node[smallcomp, fill=lime!25] (openai) at ($(provider_layer)+(1,-0.9)$) {OpenAI\\{\tiny REST API}};
        \node[smallcomp, fill=lime!25] (gemini) at ($(provider_layer)+(1,-2.1)$) {Gemini\\{\tiny REST API}};
        \node[smallcomp, fill=lime!25] (azure) at ($(provider_layer)+(1,-3.3)$) {Azure Vision\\{\tiny REST API}};
        
        % FLOWS
        \draw[flow] (main_act) -- ($(provider_mgr.west)+(0,0.3)$);
        \draw[flow] (main_act.east) -- (metrics.west);
        
        \draw[flow] (batch_act) -- ($(provider_mgr.west)+(0,-0.3)$);
        \draw[flow] (batch_act.east) -- (benchmark.west);
        
        \draw[flow] (benchmark.east) .. controls +(1.5,0) and +(1.5,0) .. (metrics.east);
        \draw[dataflow] (benchmark) -- (exporter);
        
        \draw[flow] (metrics) -- (memory_mon);
        \draw[flow] (metrics) -- (power_mon);
        
        \draw[flow] (provider_mgr.east) -- (interface_layer);
        
        \draw[flow] ($(interface_layer.east)+(0,3.3)$) -- (florence);
        \draw[flow] ($(interface_layer.east)+(0,2.1)$) -- (vitgpt2);
        \draw[flow] ($(interface_layer.east)+(0,0.9)$) -- (blip);
        \draw[flow] ($(interface_layer.east)+(0,-0.9)$) -- (openai);
        \draw[flow] ($(interface_layer.east)+(0,-2.1)$) -- (gemini);
        \draw[flow] ($(interface_layer.east)+(0,-3.3)$) -- (azure);
        
        % ANNOTATIONS
        \node[above=0.05cm of florence.north, font=\tiny, text=cyan!70!black, align=center] {Local};
        \node[below=0.05cm of azure.south, font=\tiny, text=lime!70!black, align=center] {Cloud};
        
    \end{tikzpicture}
    \caption{Architektura wysokiego poziomu aplikacji badawczej CaptionLab}
    \label{fig:captionlab_architecture}
\end{figure}

\subsubsection{Warstwa prezentacji}\label{sss:Warstwa prezentacji}
\noindent
Warstwa prezentacji stanowi interfejs użytkownika aplikacji i odpowiada za interakcję z użytkownikiem końcowym.
Składa się ona z dwóch głównych ekranów aktywności (Activities) systemu Android, z których każdy realizuje inny scenariusz badawczy.

Pierwsza, główna aktywność (MainActivity) (rysunki \ref{fig:mainactivity_1}. oraz \ref{fig:mainactivity_2}.) pozwala na interaktywne testowanie pojedynczych obrazów.
Użytkownik może wybrać dowolny obraz z galerii urządzenia lub wykonać nową fotografię za pomocą aparatu, nastepnie wskazać konkretny model AI
, z którego chce skorzystać. Po zatwierdzeniu wyboru system przeprowadza inferencję przy pomocy wybranego modelu i wyświetla wygenerowany opis 
wraz z zebranymi metrykami inferencji.

\begin{figure}[!h]
    \centering
    \begin{minipage}{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{MainActivity_1.png}
        \caption{MainActivity - ekran wyboru modeli i obrazu.}
        \label{fig:mainactivity_1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{MainActivity_2.png}
        \caption{MainActivity - wyniki generowania opisu z metrykami.}
        \label{fig:mainactivity_2}
    \end{minipage}
\end{figure}

Ten tryb jest szczególnie przydatny podczas wstepnej kalibracji systemu oraz weryfikacji poprawności działania poszczególnych 
modeli AI, przed przystąpieniem do właściwych testów wydajnościowych na większą skalę.

Druga aktywność (BatchTestActivity) (rysunki \ref{fig:batchactivity_1}. i \ref{fig:batchactivity_2}.)
stanowi fundament funkcjonalności badawczej platformy.
Pozwala ona na przeprowadzanie zautomatyzowanych testów wydajnościowych na dużym zbiorze obrazów, z możliwością wielokrotnego
powtarzania pomiarów dla większej wiarygodności wyników.

Użytkownik może załadować wcześniej przygotowany zbiór danych (np. podzbiór COCO\cite{coco_lin2014}), bądź wskazać własny katalog
z obrazami testowymi. Następnie wybiera zestaw modeli AI do przeprowadzenia badań. Co ważne system umożliwia jednoczesne testowanie
wielu modeli w ramach jednego eksperymentu, co znacząco przyspiesza proces zbierania danych. Finalnie definiuje kluczowe parametry eksperymentu,
takie jak liczba właściwych powtórzeń pomiarów, liczba serii rozgrzewkowych, maksymalny czas oczekiwania na odpowiedź oraz
format eksportu wyników, po czym uruchamia testy.

Podczas trwania eksperymentu, interfejs na bieżąco prezentuje postęp wykonywanych operacji, obejmujący informacje o aktualnym modelu, 
przetwarzanym obrazie czy aktualnej iteracji pomiaru. Po zakończeniu testów uruchamiany jest automatyczny 
proces eksportu zebranych danych zapisujący wyniki w przestrzeni dyskowej aplikacji.

\begin{figure}[!h]
    \centering
    \begin{minipage}{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BatchTestActivity_1.png}
        \caption{BatchTestActivity - wybór zbioru danych i modeli AI.}
        \label{fig:batchactivity_1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.43\textwidth}
        \centering
        \includegraphics[width=\textwidth]{BatchTestActivity_2.png}
        \caption{BatchTestActivity - konfiguracja parametrów testu i egzekucja.}
        \label{fig:batchactivity_2}
    \end{minipage}
\end{figure}


\subsubsection{Warstwa logiki biznesowej}\label{sss:Warstwa logiki biznesowej}
\noindent
Warstwa logiki biznesowej stanowi centrum koordynacyjne platformy, zarządzając dostępem do modeli AI,
organizacją testów oraz zbieraniem danych pomiarowych. Kluczowymi komponentami tej warstwy są 
\texttt{ProviderManager}, \texttt{BenchmarkRunner} oraz \texttt{MetricsCollector}, którego działanie wspierają
moduły monitorujące \texttt{MemoryMonitor} oraz \texttt{PowerMonitor}.

\texttt{ProviderManager} pełni rolę centralnego rejestru modeli AI. Odpowiada za rejestrację i dostarczanie
konkretnych implementacji providerów AI na żądanie. Umożliwia dynamiczne dodawanie nowych modeli do systemu
bez konieczności modyfikacji pozostałych komponentów aplikacji oraz zarządza cyklem życia instancji providerów.

Organizacją i przebiegiem testów zajmuje się \texttt{BenchmarkRunner}. Algorytm działania tego komponentu 
opiera się na sekwencyjnym wykonywaniu zdefiniowanych testów, zgodnie z określoną konfiguracją eksperymentu.
Dla każdego wybranego modelu w pierwszej kolejności przeprowadzana jest seria rozgrzewkowa (\textit{warm-up run})
mająca na celu ustabilizowanie warunków pomiarowych. Następnie wykonywane są właściwe iteracje pomiarowe dla każdego 
obrazu w zbiorze testowym, zbierając dane pomiarowe przy pomocy \texttt{MetricsCollector}. Proces ten jest powtarzany
dla każdego z modeli zgodnie ze zdefiniowaną konfiguracją. W ten sposób minimalizowany jest wpływ efektów 
przejściowych związanych z ładowaniem modeli do pamięci urządzenia i innych nieznaczących dla pomiarów operacji.

W trakcie działania \texttt{BenchmarkRunner} inicjuje komponent \texttt{MetricsCollector}, który realizuje kompleksowy pomiar
parametrów wydajnościowych podczas każdej inferencji. System pomiarowy opiera się na metrykach z czterech głównych kategorii: danych czasowych,
metryk pamięciowych, kosztach energetycznych działania modeli oraz kosztów wywołań API (dla modeli chmurowych).
Dokładny opis poszczególnych mechanizmów pomiarowych znajduje się w sekcji \ref{ss:System zbierania metryk}. \textquote{System zbierania metryk}.
Po zakończeniu wszystkich iteracji zebrane dane są przekazywane do modułu eksportu \texttt{DataExporter} w celu zapisania ich na dysku.

Poza tym warstwa biznesowa realizuje również mechanizmy obsługi wyjątków i błędów, zapewniając stabilność działania aplikacji.
System stale monitoruje limity przekroczenia czasu odpowiedzi modeli dla zbyt długich inferencji, rejestruje wszelkie niepowodzenia,
a także pozwala na kontynuowanie testów pomimo wystąpienia błędów w trakcie eksperymentu. Wszystkie szczegóły błędów są 
logowane i uwzględnione w końcowych wynikach eksportu.


\subsubsection{Warstwa providerów AI}\label{sss:Warstwa providerow AI}
\noindent
Warstwa providerów AI stanowi najbardziej rozbudowaną część systemu i zawiera implementacje konkretnych modeli sztucznej inteligencji 
wykorzystywanych do generowania opisów obrazów. Głównym założeniem architektonicznym tej warstwy jest zastosowanie wzorca strategii 
(\textit{Strategy Pattern}) poprzez zdefiniowanie wspólnego interfejsu \texttt{CaptioningProvider} dla różnych implementacji modeli AI.

Interfejs \texttt{CaptioningProvider} definiuje minimalny kontrakt, który musi spełniać każda implementacja provider'a modelu AI, obejmujący unikalny identyfikator
oraz asynchroniczną metodę \texttt{caption(bitmap: Bitmap)}, która przyjmuję obiekt bitmapy obrazu i zwraca strukturę \texttt{CaptionResult}
zawierającą wygenerowany opis wraz z metadanymi operacji. \textit{Listing} \ref{lst:captioning_provider_interface}. zawiera definicję tego interfejsu w języku Kotlin.

\begin{addmargin}[7mm]{0mm}
\begin{lstlisting}[
    language=Kotlin, 
    caption={Interfejs CaptioningProvider}, 
    label={lst:captioning_provider_interface},
    numbers=left,
    firstnumber=5
    ]
interface CaptioningProvider {
    val id: String
    suspend fun caption(bitmap: Bitmap): CaptionResult
}

data class CaptionResult(
    val text: String,
    val extra: Map <String, Any?> = emptyMap()
)
\end{lstlisting}
\end{addmargin}

Dzięki takiemu podejściu zapewniona jest pełna elastyczność w implementacji różnych modeli, bez konieczności modyfikacji 
wyższych warstw systemu, pozwalając aplikacji na dostęp do wyspecjalizowanych implementacji poprzez wspólny interfejs \cite{strategy_pattern_schmidt}.

Wszystkie lokalne modele AI zostały zaimplementowane przy użyciu ONNX Runtime jako silnika inferencji. Pliki modeli w formacie ONNX
są przechowywane w zasobach aplikacji i są ładowane do pamięci operacyjnej urządzenia podczas inicjalizacji providera.
Każdy lokalny provider zarządza własną sesją ONNX oraz implementuje specyficzne dla danego modelu procedury przetwarzania obrazu wejściowego
i dekodowania wygenerowanego opisu z surowych wyników inferencji.

Provider'y chmurowe komunikują się z usługami zewnętrznych dostawców AI poprzez REST API. Każdy z tych provider'ów implementuje mechanizmy autoryzacji,
obsługi zapytań HTTP oraz przetwarza odpowiedzi serwera do zgodnego formatu.

Taka architektura systemu pozwala na unifikację dostępu do modeli AI niezależnie od odmiennych technologii czy szczegółów implementacji.

\subsection{Implementacja providerów AI}\label{ss:Implementacja providerow AI}
\noindent
Implementacja poszczególnych modeli AI wymagała uwzględnienia specyfiki każdego z modeli oraz zastosowanej technologii. 
Providery modeli lokalnych muszą radzić sobie z zarządzaniem ograniczonymi zasobami urządzeń mobilnych, podczas gdy 
providery chmurowych rozwiązań muszą efektywnie obsługiwać komunikację sieciową i autoryzację dla różnych formatów API.

Silnikiem inferencji dla modeli lokalnych jest ONNX Runtime Mobile, którego wybór był podyktowany przede wszystkim szerokim wsparciem
dla różnorodnych operatorów sieci neuronowych, optymalizacjami pod kątem urządzeń mobilnych oraz łatwą integracją z aplikacjami Android.
Dzięki jego użyciu możliwe było uruchomienie modeli takich jak Florence-2, ViT-GPT2 oraz BLIP bez konieczności głębokich modyfikacji architektury sieci,
po wcześniejszym eksporcie modeli do formatu ONNX.

Model ViT-GPT2 łączy transformer wizyjny (\textit{Vision Transformer}) z modelem językowym GPT-2 w architekturze enkoder-dekoder. 
W procesie wstępnego przetwarzania obraz jest skalowany do rozdzielczości 224x224 px, normalizowany według statystyk ImageNet i konwertowany z układu NHWC (\textit{Number, Height, Width, Channels})
charakterystycznego dla Androida do układu NCHW wymaganego przez model.
Model składa się z dwóch sesji ONNX. Enkoder przetwarza obraz i generuje wektorowe reprezentacje cech wizualnych, które stanowią kontekst dla dekodera GPT-2.
Dekoder autoregresywnie generuje tokeny opisu, w każdej iteracji otrzymując poprzednio wygenerowane tokeny oraz kontekst wizualny z enkodera.
Proces kończy się po osiągnięciu maksymalnej długości sekwencji lub napotkaniu znacznika końca sekwencji, a wygenerowane tokeny są dekodowane do tekstu przez dedykowany tokenizer GPT-2.

Model BLIP stanowi rozwiązanie oparte na wielozadaniowym mechanizmie treningu wstepnego (\textit{pretraining}), łączącym zadania rozumienia i generowania opisów wizualno-językowych. 
Architektura składa się z enkodera obrazu opartego na Vision Transformer oraz dekodera tekstowego z mechanizmem uwagi krzyżowej (\textit{cross-attention}).
Obraz jest skalowany do rozdzielczości 384x384 px i normalizowany według specyficznych parametrów BLIP. Model został zaimplementowany jako dwie sesje ONNX: 
enkoder generujący stany ukryte o wymiarowości 768 oraz dekoder autoregresywny generujący tokeny opisu.
Detokenizacja jest realizowana przez tokenizer BLIP obsługujący słownik WordPiece z ponad 30,000 tokenów.

Florence-2 reprezentuje najbardziej zaawansowaną architekturę multimodalną. Model został podzielony na cztery sesje ONNX:
enkoder wizyjny, moduł embedowania tokenów, enkoder tekstowy oraz dekoder autoregresywny.
Proces inferencji rozpoczyna się od ekstrakcji cech wizualnych, które są konkatenowane z wektorowymi reprezentacjami promptu \textquote{Describe image} i przetwarzane przez enkoder tekstowy.
Dekoder następnie autoregresywnie generuje opis, wykorzystując stany ukryte z enkodera oraz maski uwagi. Ta wieloetapowa architektura pozwala Florence-2 na generowanie 
bardziej szczegółowych i bogatszych kontekstowo opisów.

Provider'y chmurowe koncentrują się na prawidłowej komunikacji z zewnętrznymi usługami AI poprzez REST API, zarządzając jedynie komunikacją sieciową, autoryzacją zapytań oraz formatowaniem danych. 

OpenAI GPT-4o mini wykorzystuje API Chat Completions, gdzie obraz jest konwertowany do formatu base64 i osadzany w żądaniu JSON jako data URL z autoryzacją Bearer Token.
Google Gemini 2.0 Flash Lite stosuje podobne podejście z odmienną strukturą API, gdzie obraz jest przesyłany jako część tablicy \texttt{parts}, a autoryzacja realizowana poprzez parametr \texttt{key} w URL.
Azure Computer Vision 4.0 jako wyspecjalizowany model analizy obrazu przesyła surowe bajty JPEG bezpośrednio w ciele żądania HTTP z autoryzacją przez nagłówek \texttt{Ocp-Apim-Subscription-Key},
zwracając opis wraz ze wskaźnikiem pewności. 

Wszystkie provider'y chmurowe implementują pomiar czasów poszczególnych etapów przetwarzania,
umożliwiając szczegółową analizę wydajności komunikacji sieciowej, a także parametry inferencji takie jak ilość zużytych tokenów.


\subsection{System zbierania metryk}\label{ss:System zbierania metryk}
\noindent
Precyzyjny pomiar parametrów wydajnościowych stanowi fundamentalny aspekt prowadzonych badań, zatem projektując platformę \textquote{CaptionLab}
zaprojektowano osobny system pomiarowy integrujący różnorodne mechanizmy pomiarowe.
System ten jest realizowany przez komponent \texttt{MetricsCollector}, który został zaprojektowany w sposób zapewniający 
minimalny wpływ na mierzone wartości oraz wysoką dokładność rejestrowanych danych.

\subsubsection{Pomiar czasu wykonania}\label{sss:Pomiar czasu}
\noindent
Metryki czasowe rejestrowane są z wykorzystaniem wysokorozdzielczego zegara systemowego Android (\texttt{System.nanoTime()}) na poziomie 
providerów, który oferuje precyzję rzędu nanosekund i jest niezależny od zmian czasu systemowego urządzenia. Pomiar obejmuje cztery
fazy przetwarzania:
\begin{itemize}
    \item \textbf{Czas wstępnego przetwarzania obrazu (\texttt{pre\_ms})} - czas od otrzymania obrazu do przygotowania go do inferencji, 
    obejmujący akcje kompresji obrazu, skalowania rozdzielczości, normalizację, konwersję przestrzeni warstw oraz alokację i inicjalizację tensorów wejściowych.
    \item \textbf{Czas inferencji modelu (\texttt{infer\_ms})} - czas trwania samej operacji inferencji w modelu AI lub czas trwania zapytania
    HTTP do momentu otrzymania odpowiedzi dla modeli chmurowych.
    \item \textbf{Czas postprocesingu wyników (\texttt{post\_ms})} - czas od zakończenia inferencji do uzyskania finalnego opisu, w tym dekodowanie
    sekwencji tokenów, parsowanie odpowiedzi JSON oraz formatowanie końcowego opisu.
    \item \textbf{Całkowity czas operacji (\texttt{e2e\_ms})} - całkowity czas od przekazania obrazu do wygenerowania opisu, stanowiący
    sumę wszystkich trzech powyższych faz.
\end{itemize}

Każdy z czasów rejestrowany jest oddzielnie, co pozwala na szczegółową analizę wydajności poszczególnych etapów przetwarzania
i jest zapisywany w milisekundach dla łatwiejszej interpretacji. 

\subsubsection{Pomiar zużycia pamięci}\label{sss:Pomiar pamieci}
\noindent
Monitoring zużycia pamięci operacyjnej realizuje moduł \texttt{MemoryMonitor}, który wykorzystuje \texttt{Android Runtime API} do zbierania danych
o wykorzystaniu pamięci przez proces aplikacji. Mechanizm pomiarowy został zaprojektowany w sposób 
minimalizujący wpływ samego procesu monitorowania na mierzone wartości, poprzez wykonywanie pomiarów w osobnym wątku o niskim 
priorytecie, działającym równolegle z wątkiem głównym aplikacji.

Proces pomiaru rozpoczyna się bezpośrednio przed wywołaniem inferencji modelu, kiedy to \texttt{MetricsCollector} uruchamia 
instancję \texttt{MemoryMonitor} i rejestruje wstępny stan pamięci procesu jako wartość bazową. Wątek pomiarowy następnie 
cyklicznie próbkuje aktualny stan pamięci, rejestrując szczytową wartość zużycia podczas całego procesu inferencji. 
Po zakończeniu operacji \texttt{MetricsCollector} pobiera z monitora dwie kluczowe metryki: szczytowe zużycie pamięci operacyjnej 
(\texttt{ramPeakMb}) oraz przyrost zużycia pamięci względem wartości bazowej. Dla modeli lokalnych system dodatkowo rejestruje 
rozmiar plików modelu w formacie ONNX (\texttt{modelSizeMb}), co pozwala na ocenę narzutu pamięciowego związanego z ładowaniem 
modelu do pamięci urządzenia. 

Wszystkie zebrane dane pamięciowe są agregowane w strukturze \texttt{BenchmarkMetrics} i przekazywane
do modułu eksportu wraz z pozostałymi parametrami wydajnościowymi. 

\subsubsection{Pomiar zużycia energii}\label{sss:Pomiar energii}
\noindent
Rejestracja zużycia energii jest realizowana przez moduł \texttt{PowerMonitor}, który wykorzystuje interfejs \texttt{BatteryManager} systemu Android
do zbierania danych o parametrach elektrycznych baterii urządzenia. Komponent jest uruchamiany równolegle z \texttt{MemoryMonitor} bezpośrednio przed rozpoczęciem inferencji
i działa w osobnym wątku asynchronicznym, cyklicznie próbkując wartości natężenia prądu oraz napięcia baterii.

System rejestruje trzy kluczowe parametry elektryczne: chwilowe natężenie prądu, średnie natężenie prądu oraz pojemność baterii, które wraz z napięciem pozwalają
na obliczenie zużycia energii podczas operacji. Po zakończeniu inferencji \texttt{MetricsCollector} zatrzymuje monitor i inicjuje proces obliczenia całkowitego zużycia
energii, wyrażonego w miliwatogodzinach (mWh). Implementacja przewiduje trzy alternatywne metody kalkulacji, stosowane hierarchicznie w zależności od dostępności danych
na urządzeniu. 

Pierwsza metoda opiera się na całkowaniu mocy chwilowej w czasie, wykorzystując szereg pomiarów napięcia i natężenia prądu:
\begin{equation}
E = \sum_{i=1}^{n-1} \frac{P_i + P_{i+1}}{2} \cdot \Delta t_i
\end{equation}
gdzie $P_i = U_i \times I_i$ jest mocą chwilową w $i$-tym pomiarze, a $\Delta t_i$ to odstęp czasu między kolejnymi próbkami.

Druga metoda wykorzystuje średnie natężenie prądu dla całego okresu pomiaru:
\begin{equation}
E = U_{avg} \times I_{avg} \times \Delta t
\end{equation}
gdzie $U_{avg}$ i $I_{avg}$ to wartości średnie napięcia i natężenia prądu, a $\Delta t$ to całkowity czas trwania inferencji.

Trzecia metoda bazuje na bezpośrednim pomiarze zmiany pojemności baterii:
\begin{equation}
E = \Delta Q \times U_{avg}
\end{equation}
gdzie $\Delta Q$ to różnica pojemności baterii między początkiem a końcem pomiaru.

Warto jednak zaznaczyć, że dokładność pomiarów energetycznych jest ograniczona przez kilka czynników.
Po pierwsze, \texttt{BatteryManager} nie zawsze udostępnia pomiary prądu chwilowego na wszystkich urządzeniach, co wymusza
wykorzystanie mniej precyzyjnych metod obliczeniowych. 
Po drugie, pomiary obejumą całkowite zużycie energii przez urządzenia, a nie tylko przez sam proces inferencji, co wprowadza 
stały narzut związany z działaniem systemu operacyjnego i innych procesów w tle.

Z tych powodów metryki zużycia energii należy traktować jako przybliżone względne wskaźniki i używać ich w porównaniach między modelami
w podobnych warunkach testowych, zamiast jako absolutnych wartości zużycia energii przez inferencję.

\subsubsection{Obliczanie kosztów wywołań API}\label{sss:Koszty API}
\noindent
Dla modeli chmurowych system automatycznie oblicza szacunkowy koszt finansowy pojedynczego wywołania API na podstawie aktualnych cenników dostawców
oraz danych o zużyciu tokenów zwracanych w odpowiedziach API. Kalkulacja kosztów jest realizowana przez metodę \texttt{estimateCost()} w komponencie 
\texttt{MetricsCollector}, która stosuje specyficzne formuły dla każdego z dostawców usług AI.

Dla modeli OpenAI GPT-4o mini oraz Google Gemini 2.5 Flash Lite, które są uniwersalnymi modelami językowymi, koszty są obliczane na podstawie liczby tokenów
wejściowych (prompt) oraz wyjściowych (completion) według formuły:
\begin{equation}
C_{total} = \frac{T_{prompt}}{10^6} \times C_{prompt} + \frac{T_{completion}}{10^6} \times C_{completion}
\end{equation}
gdzie $T_{prompt}$ i $T_{completion}$ to liczby tokenów, a $C_{prompt}$ i $C_{completion}$ to stawki cenowe za milion tokenów w USD.
Dla OpenAI stosowane są stawki \$0.15 za milion tokenów wejściowych i \$0.60 za milion tokenów wyjściowych, natomiast dla Gemini odpowiednio \$0.10 i \$0.40.

Azure Computer Vision 4.0 wykorzystuje uproszczony model cenowy oparty na liczbie przetworzonych obrazów, gdzie koszt jednostkowy wynosi:
\begin{equation}
C = \frac{1.5}{1000} \text{ USD}
\end{equation}
co odpowiada stawce \$1.50 za tysiąc przetworzonych obrazów, niezależnie od złożoności operacji czy długości wygenerowanego opisu.

Obliczone koszty są zapisywane w metryce \texttt{costUsd} struktury \texttt{BenchmarkMetrics}, co umożliwia agregację kosztów dla całych zbiorów 
testowych i porównywanie cen różnych rozwiązań chmurowych w kontekście generowanych opisów obrazów.

\subsection{Eksport i analiza danych}\label{ss:Eksport danych}
\noindent
Po przeprowadzeniu eksperymentów badawczych zebrane dane muszą zostać odpowiednio zapisane i przygotowane do dalszej analizy.
Do tego celu został zaprojektowany system eksportu danych, realizowany przez moduł \texttt{DataExporter}, który generuje wyniki 
w dwóch formatach - CSV i JSON z których każdy służy innym celom analitycznym.

Dane zapisane w formacie CSV (\textit{Comma Separated Values}) zawierają szczegółowe metryki wydajnościowe dla każdej pojedynczej inferencji.
Każdy wiersz pliku reprezenruje pojedyncze wykonanie operacji generowania opisu i zawiera następujące kolumny:
\begin{itemize}
    \item \texttt{provider\_id}     - identyfikator providera,
    \item \texttt{provider\_type}   - typ modelu - lokalny/chmurowy,
    \item \texttt{image\_id}        - identyfikator obrazu,
    \item \texttt{iteration}        - numer iteracji pomiarowej,
    \item \texttt{pre\_ms}          - czas preprocessingu,
    \item \texttt{infer\_ms}        - czas inferencji,
    \item \texttt{post\_ms}         - czas postprocessingu,
    \item \texttt{e2e\_ms}          - całkowity czas end-to-end,
    \item \texttt{ram\_peak\_mb}    - szczytowe zużycie pamięci RAM,
    \item \texttt{energy\_mwh}      - zużycie energii,
    \item \texttt{cost\_usd}        - koszt wywołania,
    \item \texttt{caption}          - wygenerowany opis,
    \item \texttt{caption\_length}  - długość opisu,
    \item \texttt{timestamp}        - znacznik czasowy pomiaru,
    \item \texttt{status}           - status operacji.
\end{itemize}
Taka struktura danych umożliwia łatwy import do narzędzi analitycznych w których
mogą być przeprowadzane bardziej zaawansowane analizy zebranych danych pomiarowych.

Dane wyeksportowane do formatu JSON (\textit{JavaScript Object Notation}) zawierają zagregowane statystyki całego eksperymentu.
Hierarchiczna struktura pliku JSON pozwala na przechowywanie zagnieżdżonych danych w zorganizowany i czytelny sposób.
Struktura pliku JSON obejmuje następujące sekcje:
\begin{itemize}
    \item \texttt{metadata}     - zawiera informacje o eksperymencie, takie jak nazwa testu, opis, użyty zbiór danych, czas rozpoczęcia i zakończenia,
    \item \texttt{device}       - zawiera szczegóły dotyczące urządzenia testowego, w tym producenta, model, wersję Androida oraz parametry sprzętowe,
    \item \texttt{statistics}   - zawiera ogólne statystyki eksperymentu, takie jak liczba przetworzonych obrazów, liczba iteracji, czy współczynnik sukcesu,
    \item \texttt{providers}    - zawiera szczegółowe metryki wydajności dla każdego z testowanych modeli AI, w tym czasy inferencji, zużycie pamięci, energii oraz koszty.
\end{itemize}
Ten format danych jest szczególnie przydatny do automatycznego przetwarzania wyników przez skrypty analizujące oraz do generowania raportów porównawczych między
różnymi eksperymentami.

Wszystkie pliki eksportu są zapisywane w katalogu \texttt{exports} w przestrzeni dyskowej aplikacji z automatycznie generowanymi nazwami
zawierającymi znacznik czasowy eksperymentu, co pozwala na łatwe zarządzanie i archiwizację wyników z wielu testów. System automatycznie tworzy
również tekstowy raport zawierający czytelną dla człowieka prezentację najważniejszych wyników oraz ewentualnych błędów.

Wyeksportowane dane stanowią podstawę do dalszej analizy generowanych opisów obrazów.  
Analiza jakościowa jest przeprowadzana poza samą platformą testową przy użyciu dedykowanych narzędzi skryptowych, gdzie wygenerowane teksty 
(\texttt{caption} z plików CSV) są porównywane z referencyjnymi opisami ze zbioru danych przy użyciu metryk jakościowych
takich jak BLEU, METEOR, ROUGE oraz CIDEr, co zostało szczegółowo omówione w rozdziale \ref{ss:Metryki jakosciowe} \textquote{Metryki jakościowe}.
